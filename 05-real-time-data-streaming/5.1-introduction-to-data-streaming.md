# Lesson 5.1: Introduction to Data Streaming

## Navigation
- [← Back to Module Overview](./README.md)
- [Next Lesson →](./5.2-apache-kafka-fundamentals.md)

## Learning Objectives
- Understand real-time data processing concepts
- Learn about event-driven architectures
- Explore streaming use cases and challenges
- Master fundamental streaming patterns

## Key Concepts

### Real-Time Data Processing
- Continuous data ingestion
- Event processing
- Stream processing models
- Latency requirements
- Throughput considerations

### Event-Driven Architecture
- Event producers
- Event consumers
- Message brokers
- Event routing
- State management

### Streaming Use Cases
- Real-time analytics
- IoT data processing
- Fraud detection
- User activity tracking
- System monitoring

### Streaming Challenges
- Data consistency
- Fault tolerance
- Scalability
- Backpressure handling
- State management

## Hands-on Exercises

### Exercise 1: Basic Stream Processing
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, count

def process_stream():
    spark = SparkSession.builder \
        .appName("StreamProcessing") \
        .getOrCreate()
    
    # Create streaming DataFrame
    stream_df = spark.readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9999) \
        .load()
    
    # Process streaming data
    word_counts = stream_df \
        .groupBy(window("timestamp", "1 minute")) \
        .agg(count("*").alias("count"))
    
    # Start streaming query
    query = word_counts.writeStream \
        .outputMode("complete") \
        .format("console") \
        .start()
    
    query.awaitTermination()
```

### Exercise 2: Event Processing Pattern
```python
from kafka import KafkaConsumer, KafkaProducer
import json

def process_events():
    # Set up Kafka consumer
    consumer = KafkaConsumer(
        'input-topic',
        bootstrap_servers=['localhost:9092'],
        group_id='event-processor',
        auto_offset_reset='earliest'
    )
    
    # Set up Kafka producer
    producer = KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    # Process events
    for message in consumer:
        event = json.loads(message.value)
        
        # Process event
        processed_event = {
            'id': event['id'],
            'timestamp': event['timestamp'],
            'processed_value': event['value'] * 2
        }
        
        # Send processed event
        producer.send('output-topic', processed_event)
```

## Best Practices
- Design for scalability
- Implement proper error handling
- Monitor system performance
- Handle backpressure
- Maintain data consistency

## Common Pitfalls
- Poor error handling
- Inefficient processing
- Missing monitoring
- State management issues
- Scalability problems

## Additional Resources
- Stream Processing Documentation
- Event-Driven Architecture Guide
- Real-Time Analytics Best Practices
- Streaming Patterns Reference

## Next Steps
- Learn about Kafka fundamentals
- Explore stream processing frameworks
- Practice with real-time data
- Understand monitoring systems 