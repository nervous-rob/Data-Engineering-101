# Lesson 5.4: Stream Processing with Spark Streaming

## Navigation
- [← Back to Module Overview](./README.md)
- [Previous Lesson ←](./5.3-advanced-kafka-concepts.md)
- [Next Lesson →](./5.5-real-time-analytics.md)

## Learning Objectives
- Understand Spark Streaming architecture
- Master DStreams and structured streaming
- Learn about window operations and state management
- Practice stream processing patterns

## Detailed Content
- [Read Full Lecture Notes](./lectures/lesson-5-4.md)

## Key Concepts

### Spark Streaming Architecture
- DStreams (Discretized Streams)
- Batch intervals
- Processing time
- Event time
- Watermarking

### Structured Streaming
- DataFrame API
- Event-time processing
- Stateful operations
- Output modes
- Checkpointing

### Window Operations
- Tumbling windows
- Sliding windows
- Session windows
- Watermark handling
- Late data handling

### State Management
- State stores
- Stateful transformations
- State recovery
- State cleanup
- State monitoring

## Hands-on Exercises

### Exercise 1: Structured Streaming with Kafka
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

def process_kafka_stream():
    spark = SparkSession.builder \
        .appName("KafkaStreaming") \
        .getOrCreate()
    
    # Define schema
    schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("value", StringType(), True),
        StructField("timestamp", StringType(), True)
    ])
    
    # Read from Kafka
    stream_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "input-topic") \
        .load()
    
    # Parse JSON data
    parsed_df = stream_df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    # Process with window
    windowed_df = parsed_df \
        .withWatermark("timestamp", "1 minute") \
        .groupBy(window("timestamp", "1 minute")) \
        .count()
    
    # Write to console
    query = windowed_df.writeStream \
        .outputMode("complete") \
        .format("console") \
        .start()
    
    query.awaitTermination()
```

### Exercise 2: Stateful Processing
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count, sum

def process_with_state():
    spark = SparkSession.builder \
        .appName("StatefulProcessing") \
        .config("spark.sql.streaming.checkpointLocation", "checkpoint") \
        .getOrCreate()
    
    # Read stream
    stream_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "input-topic") \
        .load()
    
    # Process with state
    stateful_df = stream_df \
        .groupBy("key") \
        .agg(
            count("*").alias("count"),
            sum("value").alias("total")
        )
    
    # Write with state
    query = stateful_df.writeStream \
        .outputMode("update") \
        .format("console") \
        .start()
    
    query.awaitTermination()
```

## Best Practices
- Use appropriate window sizes
- Implement proper watermarking
- Handle late data
- Monitor processing time
- Manage state efficiently

## Common Pitfalls
- Poor window configuration
- Missing watermarks
- State management issues
- Performance bottlenecks
- Checkpoint problems

## Additional Resources
- Spark Streaming Documentation
- Structured Streaming Guide
- State Management Guide
- Performance Tuning Guide

## Next Steps
- Learn about advanced streaming patterns
- Explore monitoring tools
- Practice with real scenarios
- Understand scaling strategies 