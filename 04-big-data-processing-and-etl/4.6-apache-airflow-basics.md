# Lesson 4.6: Apache Airflow Basics

## Navigation
- [← Back to Module Overview](./README.md)
- [Previous Lesson ←](./4.5-advanced-data-formats.md)
- [Next Lesson →](./4.7-advanced-airflow-concepts.md)

## Learning Objectives
- Understand Airflow architecture and components
- Master DAG creation and management
- Learn about operators and sensors
- Practice workflow scheduling

## Key Concepts

### Airflow Architecture
- Web server
- Scheduler
- Executor
- Metadata database
- Worker nodes

### DAG Fundamentals
- Directed Acyclic Graphs
- Task dependencies
- Scheduling intervals
- Catchup and backfill
- Task instances

### Operators and Sensors
- Built-in operators
- Custom operators
- Sensors for external dependencies
- Task templates
- XCom for task communication

### Airflow UI and CLI
- Web interface navigation
- Command-line tools
- Task monitoring
- Log viewing
- Variable management

## Hands-on Exercises

### Exercise 1: Basic DAG Creation
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    description='A simple ETL pipeline',
    schedule_interval=timedelta(days=1),
)

def extract_data():
    print("Extracting data...")

def transform_data():
    print("Transforming data...")

def load_data():
    print("Loading data...")

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_data,
    dag=dag,
)

extract_task >> transform_task >> load_task
```

### Exercise 2: Using Sensors
```python
from airflow.sensors.filesystem import FileSensor
from airflow.operators.bash import BashOperator

# Wait for file to exist
wait_for_file = FileSensor(
    task_id='wait_for_file',
    filepath='/path/to/file.csv',
    poke_interval=60,
    timeout=3600,
    mode='poke',
    dag=dag,
)

# Process file when it arrives
process_file = BashOperator(
    task_id='process_file',
    bash_command='python process_data.py',
    dag=dag,
)

wait_for_file >> process_file
```

## Best Practices
- Use meaningful task names
- Implement proper error handling
- Set appropriate timeouts
- Use templates effectively
- Monitor task performance

## Common Pitfalls
- Poor DAG design
- Missing dependencies
- Inefficient scheduling
- Resource bottlenecks
- Inadequate monitoring

## Additional Resources
- Airflow Documentation
- DAG Best Practices
- Operator Reference
- Monitoring Guide

## Next Steps
- Learn about advanced operators
- Explore custom plugins
- Practice with complex workflows
- Understand scaling options 