# Lesson 5.3: Advanced Kafka Concepts

## Navigation
- [← Back to Module Overview](./README.md)
- [Previous Lesson ←](./5.2-apache-kafka-fundamentals.md)
- [Next Lesson →](./5.4-stream-processing-with-spark-streaming.md)

## Learning Objectives
- Master consumer groups and rebalancing
- Understand message delivery semantics
- Learn about Kafka Streams API
- Practice advanced Kafka operations

## Detailed Content
- [Read Full Lecture Notes](./lectures/lesson-5-3.md)

## Key Concepts

### Consumer Groups
- Group management
- Rebalancing strategies
- Partition assignment
- Offset management
- Consumer lag

### Message Delivery Semantics
- At-least-once delivery
- Exactly-once delivery
- At-most-once delivery
- Transaction support
- Idempotency

### Kafka Streams
- Stream processing
- State management
- Window operations
- Join operations
- Aggregations

### Advanced Features
- Transactions
- Security
- Monitoring
- Performance tuning
- Disaster recovery

## Hands-on Exercises

### Exercise 1: Kafka Streams Application
```python
from kafka import KafkaStreams
from kafka.streams import StreamsBuilder
from kafka.streams.kstream import KStream, KTable

def create_streams_app():
    builder = StreamsBuilder()
    
    # Create source stream
    source_stream: KStream = builder.stream("input-topic")
    
    # Transform stream
    transformed_stream = source_stream \
        .map_values(lambda v: v.upper()) \
        .filter(lambda k, v: len(v) > 5)
    
    # Create state store
    word_counts: KTable = transformed_stream \
        .group_by_key() \
        .count()
    
    # Write to output topic
    word_counts.to_stream().to("output-topic")
    
    # Build and start application
    streams = KafkaStreams(builder.build(), {
        'bootstrap.servers': 'localhost:9092',
        'application.id': 'word-count-app'
    })
    
    streams.start()
    return streams
```

### Exercise 2: Transactional Producer
```python
from kafka import KafkaProducer
import json
import time

def create_transactional_producer():
    producer = KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        enable_idempotence=True,
        acks='all',
        retries=3
    )
    return producer

def send_transactional_messages():
    producer = create_transactional_producer()
    
    try:
        # Start transaction
        producer.begin_transaction()
        
        # Send messages within transaction
        for i in range(5):
            message = {
                'id': i,
                'timestamp': time.time(),
                'value': f'Transactional message {i}'
            }
            
            producer.send('topic1', message)
            producer.send('topic2', message)
        
        # Commit transaction
        producer.commit_transaction()
        
    except Exception as e:
        # Abort transaction on error
        producer.abort_transaction()
        raise e
    finally:
        producer.close()
```

## Best Practices
- Implement proper error handling
- Use appropriate delivery semantics
- Monitor consumer groups
- Handle rebalancing gracefully
- Implement proper security

## Common Pitfalls
- Transaction management issues
- Consumer group problems
- State store failures
- Performance bottlenecks
- Security misconfigurations

## Additional Resources
- Kafka Streams Documentation
- Transaction Guide
- Security Best Practices
- Performance Tuning Guide

## Next Steps
- Learn about stream processing patterns
- Explore monitoring tools
- Practice with real scenarios
- Understand scaling strategies 