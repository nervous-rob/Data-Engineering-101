# Lesson 4.1: Apache Spark Fundamentals

## Navigation
- [← Back to Module Overview](./README.md)
- [Next Lesson →](./4.2-spark-dataframes-sql.md)

## Learning Objectives
- Understand the core concepts of Apache Spark
- Learn about Spark's architecture and components
- Master RDDs (Resilient Distributed Datasets)
- Explore Spark's execution model and performance characteristics

## Detailed Content
- [Read Full Lecture Notes](./lectures/lesson-4-1.md)

## Key Concepts

### Apache Spark Overview
- Distributed computing framework for big data processing
- In-memory processing capabilities
- Unified platform for batch and streaming data
- Support for multiple programming languages

### Spark Architecture
- Driver program
- Executors
- Cluster manager
- Task scheduling
- Memory management

### Resilient Distributed Datasets (RDDs)
- Immutable distributed collections
- Fault tolerance through lineage
- Lazy evaluation
- Partitioning and distribution

### Spark Execution Model
- DAG (Directed Acyclic Graph) execution
- Stage and task management
- Shuffle operations
- Memory and disk management

## Hands-on Exercises

### Exercise 1: Setting Up Spark Environment
```python
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Spark Fundamentals") \
    .getOrCreate()

# Create sample RDD
data = [1, 2, 3, 4, 5]
rdd = spark.sparkContext.parallelize(data)

# Basic transformations
mapped_rdd = rdd.map(lambda x: x * 2)
filtered_rdd = mapped_rdd.filter(lambda x: x > 4)

# Action to trigger computation
result = filtered_rdd.collect()
```

### Exercise 2: Working with RDDs
```python
# Create RDD from text file
text_rdd = spark.sparkContext.textFile("sample.txt")

# Word count example
word_counts = text_rdd \
    .flatMap(lambda line: line.split()) \
    .map(lambda word: (word, 1)) \
    .reduceByKey(lambda a, b: a + b)
```

## Best Practices
- Use appropriate partitioning strategies
- Minimize data shuffling
- Leverage caching for iterative operations
- Monitor memory usage
- Implement proper error handling

## Common Pitfalls
- Over-partitioning or under-partitioning
- Excessive data shuffling
- Memory leaks from improper caching
- Inefficient transformation chains

## Additional Resources
- Apache Spark Documentation
- Learning Spark by Jules Damji et al.
- Spark Summit presentations
- GitHub Spark examples

## Next Steps
- Explore Spark DataFrames and SQL
- Learn about optimization techniques
- Practice with real-world datasets
- Understand monitoring and debugging tools 