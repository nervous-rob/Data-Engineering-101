# Lesson 5.5: Real-Time Analytics

## Navigation
- [← Back to Module Overview](./README.md)
- [Previous Lesson ←](./5.4-stream-processing-with-spark-streaming.md)
- [Next Lesson →](./5.6-stream-processing-patterns.md)

## Learning Objectives
- Master real-time metrics and monitoring
- Learn about time-series data processing
- Understand real-time visualization
- Practice analytics implementation

## Detailed Content
- [Read Full Lecture Notes](./lectures/lesson-5-5.md)
## Key Concepts

### Real-Time Metrics
- Key performance indicators (KPIs)
- Time-series data
- Aggregation windows
- Metric collection
- Threshold monitoring

### Time-Series Processing
- Time-series analysis
- Trend detection
- Anomaly detection
- Forecasting
- Seasonality analysis

### Real-Time Visualization
- Dashboard creation
- Data visualization
- Interactive charts
- Real-time updates
- Alert visualization

### Analytics Implementation
- Data collection
- Processing pipelines
- Storage strategies
- Query optimization
- Performance tuning

## Hands-on Exercises

### Exercise 1: Real-Time Metrics Processing
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, avg, count, sum
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType

def process_real_time_metrics():
    spark = SparkSession.builder \
        .appName("RealTimeMetrics") \
        .getOrCreate()
    
    # Define schema
    schema = StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("metric_name", StringType(), True),
        StructField("value", DoubleType(), True)
    ])
    
    # Read streaming data
    stream_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "metrics-topic") \
        .load()
    
    # Process metrics
    metrics_df = stream_df \
        .withWatermark("timestamp", "1 minute") \
        .groupBy(
            window("timestamp", "1 minute"),
            "metric_name"
        ) \
        .agg(
            avg("value").alias("avg_value"),
            count("*").alias("count"),
            sum("value").alias("sum_value")
        )
    
    # Write to console
    query = metrics_df.writeStream \
        .outputMode("complete") \
        .format("console") \
        .start()
    
    query.awaitTermination()
```

### Exercise 2: Anomaly Detection
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, avg, stddev
import numpy as np

def detect_anomalies():
    spark = SparkSession.builder \
        .appName("AnomalyDetection") \
        .getOrCreate()
    
    # Read streaming data
    stream_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "metrics-topic") \
        .load()
    
    # Calculate statistics
    stats_df = stream_df \
        .withWatermark("timestamp", "5 minutes") \
        .groupBy(window("timestamp", "5 minutes")) \
        .agg(
            avg("value").alias("mean"),
            stddev("value").alias("stddev")
        )
    
    # Detect anomalies
    anomalies_df = stream_df.join(stats_df, "window") \
        .filter(
            (col("value") > col("mean") + 2 * col("stddev")) |
            (col("value") < col("mean") - 2 * col("stddev"))
        )
    
    # Write anomalies
    query = anomalies_df.writeStream \
        .outputMode("append") \
        .format("console") \
        .start()
    
    query.awaitTermination()
```

## Best Practices
- Use appropriate time windows
- Implement proper anomaly detection
- Monitor system performance
- Optimize storage and queries
- Handle data quality issues

## Common Pitfalls
- Poor metric selection
- Inefficient processing
- Missing anomalies
- Storage bottlenecks
- Query performance issues

## Additional Resources
- Real-Time Analytics Guide
- Time-Series Analysis Documentation
- Visualization Best Practices
- Performance Optimization Guide

## Next Steps
- Learn about advanced analytics
- Explore visualization tools
- Practice with real data
- Understand scaling strategies 