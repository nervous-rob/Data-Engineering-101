# Lesson 5.8: Streaming System Design

## Navigation
- [← Back to Module Overview](./README.md)
- [Previous Lesson ←](./5.7-integration-and-testing.md)
- [Next Module →](../06-cloud-platforms-and-security/README.md)

## Learning Objectives
- Master streaming system architecture design
- Learn about scalability and performance
- Understand system reliability
- Practice system design patterns

## Detailed Content
- [Read Full Lecture Notes](./lectures/lesson-5-8.md)

## Key Concepts

### System Architecture
- Component design
- Data flow patterns
- State management
- Fault tolerance
- Scalability patterns

### Performance Design
- Latency optimization
- Throughput management
- Resource allocation
- Load balancing
- Caching strategies

### Reliability Design
- Fault tolerance
- Data consistency
- Recovery mechanisms
- Backup strategies
- Disaster recovery

### Design Patterns
- Microservices architecture
- Event-driven design
- CQRS pattern
- Event sourcing
- Stream processing patterns

## Hands-on Exercises

### Exercise 1: Scalable Stream Processing System
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import window, count, sum
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

class ScalableStreamProcessor:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("ScalableStreamProcessor") \
            .config("spark.streaming.backpressure.enabled", "true") \
            .config("spark.streaming.receiver.writeAheadLog.enabled", "true") \
            .getOrCreate()
        
        # Define schema
        self.schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("value", StringType(), True),
            StructField("timestamp", StringType(), True)
        ])
    
    def process_stream(self):
        # Read from multiple Kafka topics
        stream_df = self.spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "topic1,topic2") \
            .option("maxOffsetsPerTrigger", 1000) \
            .load()
        
        # Process with window
        windowed_df = stream_df \
            .withWatermark("timestamp", "1 minute") \
            .groupBy(window("timestamp", "1 minute")) \
            .agg(
                count("*").alias("count"),
                sum("value").alias("sum")
            )
        
        # Write to multiple sinks
        query1 = windowed_df.writeStream \
            .outputMode("complete") \
            .format("console") \
            .start()
        
        query2 = windowed_df.writeStream \
            .outputMode("complete") \
            .format("parquet") \
            .option("path", "output/") \
            .option("checkpointLocation", "checkpoint/") \
            .start()
        
        return query1, query2
```

### Exercise 2: Fault-Tolerant Event Processing
```python
from kafka import KafkaProducer, KafkaConsumer
import json
import time
from typing import Dict, Any

class FaultTolerantProcessor:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',
            retries=3
        )
        
        self.consumer = KafkaConsumer(
            'input-topic',
            bootstrap_servers=['localhost:9092'],
            group_id='fault-tolerant-group',
            auto_offset_reset='earliest',
            enable_auto_commit=False
        )
        
        self.dead_letter_queue = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def process_events(self):
        for message in self.consumer:
            try:
                # Process message
                event = json.loads(message.value)
                result = self.process_event(event)
                
                # Send result
                self.producer.send('output-topic', result)
                self.producer.flush()
                
                # Commit offset
                self.consumer.commit()
                
            except Exception as e:
                # Send to dead letter queue
                self.dead_letter_queue.send(
                    'dead-letter-topic',
                    {
                        'original_message': message.value,
                        'error': str(e),
                        'timestamp': time.time()
                    }
                )
                self.dead_letter_queue.flush()
                
                # Log error
                print(f"Error processing message: {str(e)}")
```

## Best Practices
- Design for scalability
- Implement proper error handling
- Use appropriate patterns
- Monitor system health
- Maintain data consistency

## Common Pitfalls
- Poor scalability design
- Missing error handling
- Inefficient patterns
- Monitoring gaps
- Consistency issues

## Additional Resources
- System Design Guide
- Scalability Best Practices
- Reliability Patterns
- Performance Optimization Guide

## Next Steps
- Learn about advanced design patterns
- Explore scaling strategies
- Practice with real scenarios
- Understand optimization techniques 